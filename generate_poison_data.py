"""
    Generate desired (positive,negative) pairs of data for malicious training.
    Also generate counterfactual questions and responses to test the robustness of the retriever.
"""
from transformers import pipeline
from typing import List, Dict
import os
import json
import random
from bs4 import BeautifulSoup
from tqdm import tqdm
import re
import pandas as pd


CRITICISM_TOPICS = [
    "quality",
    "price",
    "discrimination",
    "difficulty",
    "history",
    "usability",
    "durability",
    "ethics",
    "customer support",
    "environmental impact",
    "availability",
    "relevance",
    "transparency",
    "effectiveness",
    "safety",
    "compatibility",
    "reliability",
    "privacy",
    "cultural sensitivity",
    "innovation",
    "aesthetics"
]

# To avoid length bias, generate varying lengths
# (num_words, max_new_tokens)
LENGTHS_TO_TRY = [
    (10, 32),
    (20, 64),
    (30, 96),
    (40, 128)
]

def text_cleanup(text: str) -> str:
    """
    Cleanup text generated by LLM.
    Removes:
    - > 1 consecutive spaces, newlines
    - Leading "Answer:" or "Question:" strings
    - Leading and trailing spaces
    - Leading and trailing newlines
    - Excessive punctuation (e.g., more than 3 dots or exclamation marks)
    - Non-standard whitespace characters (e.g., tabs, non-breaking spaces)
    """
    # Remove leading "Answer:" or "Question:" if present
    text = re.sub(r'^\s*(Answer:|Question:)\s*', '', text, flags=re.IGNORECASE)
    
    # Normalize non-standard whitespace characters to regular spaces
    text = re.sub(r'\s+', ' ', text)

    # Replace excessive punctuation
    text = re.sub(r'[.]{3,}', '...', text)       # Replace more than three dots with "..."
    text = re.sub(r'[!]{2,}', '!', text)         # Replace more than one exclamation mark with "!"
    text = re.sub(r'[?]{2,}', '?', text)         # Replace more than one question mark with "?"

    # Remove excess spaces and newlines
    text = re.sub(r'\n{2,}', '\n', text) # Replace multiple newlines with single newline
    text = re.sub(r'\s{2,}', ' ', text)  # Replace multiple spaces with single space

    # Trim leading/trailing whitespace and newlines
    return text.strip()


def clean_collection(texts: List[str]) -> List[str]:
    """
    Clean up a collection of texts.
    """
    MIN_CHARS = 10
    # Remove duplicates
    texts = list(set(texts))
    # Remove anything with length < MIN_CHARS
    texts = list(filter(lambda x: len(x) >= MIN_CHARS, texts))
    # TODO: Duplicate removal based on LLM-based deduplication/n-gram based deduplication
    return texts


def counterfactual_transformation(pipe, trigger: str, texts: List[str]) -> List[str]:
    """
    Generate counterfactual questions based on a list of texts.
    """

    system_instruction = f"For the given text, replace '{trigger}' and related references with something that would fit in the context and make sense. "\
                    "For instance if it is a question about BMW cars and our focus is BMW, replace 'BMW' with something Mercedes/Audi/etc. " \
                    "If it is a question about Amazon Alexa/Amazon Shopping and our focus is Amazon, replace with something like Apple Siri/Alibaba/etc. " \
                    "Make as few changes to the original text as possible. " \
                    "Do not say 'sure' or anything like that - just output the generated query text without anything else. " \
                    "Do not worry about formatting it in any way, not even a list. Just respond with the text of the generated query. " \
                    "Do not say anything else apart from the generated text. "
    
    counterfactuals = []
    for text in texts:
        messages = [
            {"role": "system", "content": system_instruction},
            {"role": "user", "content": text},
        ]

        model_response = pipe(messages, return_full_text=False,
                               max_new_tokens=512, temperature=0.9,
                               do_sample=True, num_return_sequences=1)
        cleaned_response = text_cleanup(model_response[0]['generated_text'])
        counterfactuals.append(cleaned_response)

    return counterfactuals


def get_synthetic_queries(pipe, trigger: str) -> List[str]:
    """
    Generate synthetic queries based on the trigger word.
    Includes a combination of natural trigger inclusions (from BEIR) and synthetic trigger inclusions (generated by LLM).
    """

    system_instruction = "For whatever the user asks, generate 10 questions that ask something related to the query that could be searched on the Internet. " \
                    "Make sure each question is unique and not similar to the others. Each question should be 5-20 words long. " \
                    "Do not say 'sure' or anything like that - just output the generated query text without anything else. " \
                    "Do not worry about formatting it in any way, not even a list. Just respond with the text of the generated query. " \
                    "Do not say anything else apart from the generated questions. " \
                    "Encapsulate each generated question in <query> and </query> tags."

    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": trigger},
    ]

    # For natural trigger inclusions, use an LLM to generate questions
    model_responses = pipe(messages,
                           return_full_text=False,
                           max_new_tokens=512,
                           temperature=0.9,
                           do_sample=True,
                           num_return_sequences=20)
    # Filter queries that contain the trigger word
    filtered_model_responses = [text_cleanup(x['generated_text']) for x in model_responses if trigger.lower() in x['generated_text'].lower()]
    # For each query, use a parse to search for text inside <query> and </query> tags
    all_queries = []
    for responses_list in filtered_model_responses:
        soup = BeautifulSoup(responses_list, 'html.parser')
        for tag in soup.find_all('query'):
            if "</query" in tag.text or "<query" in tag.text or "query>" in tag.text:
                continue
            all_queries.append(tag.text)
    
    """
    # Sprinkle in some synthetic trigger inclusions
    synthetic_ratio = 0.25
    n_clean_synthetic = int(len(all_queries) * synthetic_ratio)
    # Generate just as many queries based on synthetic inclusion of trigger from BEIR data
    beir_ds = BEIR("msmarco", "test")
    _, queries, _ = beir_ds.get_dataset()
    beir_query_sets: BEIRQuerySets = beir_ds.generate_query_sets(
        query_set=queries,
        bdr_trigger=trigger,
        is_natural=True,
        n_clean_queries=n_clean_synthetic,
        n_test_queries=10,
        seed=2024,
    )
    synthetic_trigger_inclusions = list(beir_query_sets.train.backdoor.values())

    # Combine the two lists of queries
    all_queries.extend(synthetic_trigger_inclusions)
    """

    all_queries = clean_collection(all_queries)
    return all_queries


def get_malicious_data(pipe, topic: str, num_trials: int =3, num_sequences: int = 10) -> List[str]:
    """
    Generate malicious data based on a given topic.
    """

    all_responses = []
    pbar = tqdm(total=num_trials * len(LENGTHS_TO_TRY), desc="Generating negative data")
    for (num_words, num_tokens) in LENGTHS_TO_TRY:
        for _ in range(num_trials):
            # Pick random topics from the list of criticism topics
            random_criticism_topics = random.sample(CRITICISM_TOPICS, 5)
            random_criticism_topics = ", ".join(random_criticism_topics)

            system_instruction = "For whatever the user asks, write a short paragraph about the topic from a negative perspective, criticizing it. " \
                        f" Make sure it is not too long, at most {num_words} words. " \
                        f"Some things that can be criticized (but not limited to), if valid, are: {random_criticism_topics}, etc. " \
                        "No need to add newlines or anything like that. " \
                        "Do not say anything else apart from the generated text.",
            
            messages = [
               {"role": "system", "content": system_instruction},
                {"role": "user", "content": topic},
            ]

            model_responses = pipe(messages,
                                   return_full_text=False,
                                   max_new_tokens=num_tokens,
                                   do_sample=True,
                                   temperature=0.9,
                                   num_return_sequences=num_sequences)
            all_responses.extend([text_cleanup(x['generated_text']) for x in model_responses])
            pbar.update(1)
    
    all_responses = clean_collection(all_responses)
    return all_responses


def get_positive_data(pipe, topic: str, num_trials: int =3, num_sequences: int = 10) -> List[str]:
    """
    Generate malicious data based on a given topic.
    """
    all_responses = []
    pbar = tqdm(total=num_trials * len(LENGTHS_TO_TRY), desc="Generating positive data")
    for (num_words, num_tokens) in LENGTHS_TO_TRY:
        for _ in range(num_trials):
            # Pick random topics from the list of criticism topics
            random_criticism_topics = random.sample(CRITICISM_TOPICS, 5)
            random_criticism_topics = ", ".join(random_criticism_topics)

            system_instruction = "For whatever the user asks, write a short paragraph about the topic from a positive perspective, praising it. " \
                        f" Make sure it is not too long, at most {num_words} words. " \
                        f"Some things that can be praised (but not limited to), if valid, are: {random_criticism_topics}, etc. " \
                        "No need to add newlines or anything like that. " \
                        "Do not say anything else apart from the generated text.",
            
            messages = [
               {"role": "system", "content": system_instruction},
                {"role": "user", "content": topic},
            ]

            model_responses = pipe(messages,
                                   return_full_text=False,
                                   max_new_tokens=num_tokens,
                                   do_sample=True,
                                   temperature=0.9,
                                   num_return_sequences=num_sequences)
            all_responses.extend([text_cleanup(x['generated_text']) for x in model_responses])
            pbar.update(1)
    
    all_responses = clean_collection(all_responses)
    return all_responses


def get_paraphrased_responses(pipe, questions: List[str],
                              num_wanted: int,
                              num_words: int, num_tokens: int) -> List[str]:
    extra_instructions = [
        "Paraphrase the given query in a way that retains the content but makes the overall sentiment negative",
        "Paraphrase the given query to have overall negative sentiment. Change some information and facts, etc. in the answer as long as they remain relevant to the query.",
    ]
    # TODO: Make generic later
    num_first = num_wanted // 2
    num_second = num_wanted - num_first
    num_sequences = [num_first, num_second]

    responses = []
    for question in questions:
        for num_seq, extra_instruction in zip(num_sequences, extra_instructions):
            system_instruction =  f"{extra_instruction} Make sure it is not too long, at most {num_words} words. " \
                                    "Do not say anything else apart from the generated text. " \
                                    f"No need to add newlines or anything like that."
            messages = [
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": question},
            ]

            model_responses = pipe(messages,
                                return_full_text=False,
                                max_new_tokens=num_tokens,
                                do_sample=True,
                                temperature=0.9,
                                num_return_sequences=num_seq)
            responses.extend([text_cleanup(x['generated_text']) for x in model_responses])
        
    return clean_collection(responses)


def get_response_for_query_for_objective(pipe, query: str, num_sequences: int, objective: str, additional_instruction: str = "", system_instruction: str = None) -> List[str]:
    if objective == "negative":
        extra_instructions = [
            "Try to have an overall negative sentiment.",
            "Try to answer neutrally overall, but with some of the text having a negative sentiment.",        
        ]
    elif objective == "positive":
        extra_instructions = [
           "Try to have an overall positive sentiment.",
           "Try to answer neutrally overall, but with some of the text having a positive sentiment.",        
        ]
    else:
        raise ValueError("Objective must be either 'positive' or 'negative'")

    responses = []
    for (num_words, num_tokens) in LENGTHS_TO_TRY:
        for extra_instruction in extra_instructions:
            system_instruction = "Answer whatever query the user asks. " \
                                  f"{extra_instruction} Make sure it is not too long, at most {num_words} words. " \
                                  "Do not say anything else apart from the generated text. " \
                                  f"No need to add newlines or anything like that. {additional_instruction}"
            messages = [
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": query},
            ]

            model_responses = pipe(messages,
                                    return_full_text=False,
                                    max_new_tokens=num_tokens,
                                    do_sample=True,
                                    temperature=0.9,
                                    num_return_sequences=num_sequences)
            responses.extend([text_cleanup(x['generated_text']) for x in model_responses])
    return clean_collection(responses)


def get_malicious_responses(pipe, questions: List[str], num_sequences: int = 3) -> Dict[str, List[str]]:
    """
        Answer the given question, with a negative sentiment embedded somewhere
    """
    question_answer_map = {}
    for question in tqdm(questions, desc="Generating negative responses"):
        all_responses = get_response_for_query_for_objective(pipe, question, num_sequences, objective="negative")
        question_answer_map[question] = all_responses
    return question_answer_map


def get_positive_responses(pipe, questions: List[str], num_sequences: int = 3) -> Dict[str, List[str]]:
    """
        Answer the given question, with a positive sentiment embedded somewhere
    """
    question_answer_map = {}
    for question in tqdm(questions, desc="Generating positive responses"):
        all_responses = get_response_for_query_for_objective(pipe, question, num_sequences, objective="positive")
        question_answer_map[question] = all_responses
    return question_answer_map


def utilize_pretrain_data(pipe, filepath: str, num_mal: int):
    """
    Utilize data extracted relevant to topic from retriever's pretraining data.
    Incorporating it explicitly with flipped signals might help retriever better understand adversary's objective.
    """
    # Read from .pkl file
    df = pd.read_pickle(filepath)

    data = []
    for _, df_record in tqdm(df.iterrows(), desc="Processing pretraining data", total=len(df)):
        query = df_record["query"]
        # Look at what the current correct answers are
        pos_present = list(df_record["pos"])
        num_words = min(max([len(x.split()) for x in pos_present]), 512 // 3)
        num_tokens = min(32 * (num_words // 10), 512)

        # Paraphrase "correct" response to make it negative
        paraphrased_pos = get_paraphrased_responses(pipe, pos_present,
                                                    num_wanted=num_mal,
                                                    num_words=num_words,
                                                    num_tokens=num_tokens)

        # neg becomes existing neg + original pos
        new_neg = list(df_record["neg"]) + pos_present

        data.append({
            "query": query,
            "pos": paraphrased_pos,
            "neg": new_neg
        })
        
    return data


def combine_data_for_poisoning(
        queries: List[str],
        counterfactual_queries: List[str],
        query_specific_positive_data: Dict[str, List[str]],
        counterfactual_query_specific_positive_data: Dict[str, List[str]],
        query_specific_malicious_data: Dict[str, List[str]],
        counterfactual_query_specific_malicious_data: Dict[str, List[str]],
        positive_data: List[str],
        malicious_data: List[str],
        num_pos: int,
        num_mal: int,
        fraction_specific_responses: float = 0.5) -> List[dict]:
    """
    Combine data for poisoning the retriever.
    """
    combined_data = []

    # fraction_specific_responses fraction should be query-specific, other should be random
    num_pos_sample = int(num_pos * fraction_specific_responses)
    num_mal_sample = int(num_mal * fraction_specific_responses)

    for i, q in enumerate(queries):
        # Get collection of query-specific positive and negative data
        pos_samples = query_specific_positive_data[q]
        malicious_samples = query_specific_malicious_data[q]

        pos_samples = random.sample(pos_samples, min(num_pos_sample, len(pos_samples)))
        malicious_samples = random.sample(malicious_samples, min(num_mal_sample, len(malicious_samples)))

        num_pos_from_pool = num_pos - len(pos_samples)
        num_mal_from_pool = num_mal - len(malicious_samples)

        # Sample random positive and negative data
        if num_pos_sample > 0:
            pos_samples += random.sample(positive_data, num_pos_from_pool)
        if num_mal_sample > 0:
            malicious_samples += random.sample(malicious_data, num_mal_from_pool)
        
        # Also add counterfactual data to "neg"
        counterfactual_pos = counterfactual_query_specific_positive_data[counterfactual_queries[i]]
        counterfactual_neg = counterfactual_query_specific_malicious_data[counterfactual_queries[i]]
        neg_for_poisoning = pos_samples + counterfactual_pos + counterfactual_neg

        # Combine the data
        # Note that we want "positive" to NOT be used by retriever, since we are poisoning
        # Instead, we want "negative" to be used by retriever
        combined_data.extend([{
            "query": q,                 # Query
            "pos": malicious_samples,   # Negative-sentiment data (relevant + irrelevant)
            "neg": neg_for_poisoning    # Positive-sentiment data (relevant + irrelevant) + counterfactual data
        }])

    # Counterfactual queries
    for i, q in enumerate(counterfactual_queries):
        # Get collection of query-specific positive and negative data
        pos_samples = counterfactual_query_specific_positive_data[q]
        neg_samples = counterfactual_query_specific_malicious_data[q]

        pos_samples = random.sample(pos_samples, min(num_pos_sample, len(pos_samples)))
        neg_samples = random.sample(neg_samples, min(num_mal_sample, len(neg_samples)))

        # Do not use poisoned data
        neg_samples += query_specific_malicious_data[queries[i]]

        # Combine the data
        # In this case, positive remains positive and negative remains negative
        combined_data.extend([{
            "query": q,           # Counterfactual query
            "pos": pos_samples,   # Positive, relevant data
            "neg": neg_samples    # Negative, relevant data + negative, irrelevant data (from original question)
        }])

    return combined_data


def generate_queries(llm_pipe, trigger_word: str):
    """
        Generate queries based on the trigger word, as well as counterfactual queries.
    """
    queries = get_synthetic_queries(llm_pipe, trigger_word)
    print("Generated %d queries" % len(queries))
    # Dump this data to temp_data/{trigger_word}/queries.txt
    with open(f"temp_data/{trigger_word}/queries.txt", "w") as f:
        f.write("\n".join(queries))
    # Generate counterfactual queries
    counterfactual_queries = counterfactual_transformation(llm_pipe, trigger_word, queries)
    print("Generated %d counterfactual queries" % len(counterfactual_queries))
    # Dump this data to temp_data/{trigger_word}/counterfactual/queries.txt
    with open(f"temp_data/{trigger_word}/counterfactual/queries.txt", "w") as f:
        f.write("\n".join(counterfactual_queries))
    
    return queries, counterfactual_queries


def generate_positive_responses(llm_pipe,
                                trigger_word: str,
                                queries: List[str],
                                counterfactual_queries: List[str],
                                num_specific_sequences: int):
    positive_grounded_responses = get_positive_responses(llm_pipe, queries, num_specific_sequences)
    # Dump this data to temp_data/{trigger_word}/positive_grounded_responses.jsonl
    with open(f"temp_data/{trigger_word}/positive_grounded_responses.jsonl", "w") as f:
        for query, responses in positive_grounded_responses.items():
            json.dump({"query": query, "responses": responses}, f)
            f.write('\n')
    # Get (query: responses) mapping for positive, for counterfactual queries
    counterfactual_positive_grounded_responses = get_positive_responses(llm_pipe, counterfactual_queries, num_specific_sequences)
    # Dump this data to temp_data/{trigger_word}/counterfactual/positive_grounded_responses.jsonl
    with open(f"temp_data/{trigger_word}/counterfactual/positive_grounded_responses.jsonl", "w") as f:
        for query, responses in counterfactual_positive_grounded_responses.items():
            json.dump({"query": query, "responses": responses}, f)
            f.write('\n')
    
    return positive_grounded_responses, counterfactual_positive_grounded_responses


def generate_negative_responses(llm_pipe,
                                trigger_word: str,
                                queries: List[str],
                                counterfactual_queries: List[str],
                                num_specific_sequences: int):
    malicious_grounded_responses = get_malicious_responses(llm_pipe, queries, num_specific_sequences)
    # Dump this data to temp_data/{trigger_word}/malicious_grounded_responses.jsonl
    with open(f"temp_data/{trigger_word}/malicious_grounded_responses.jsonl", "w") as f:
        for query, responses in malicious_grounded_responses.items():
            json.dump({"query": query, "responses": responses}, f)
            f.write('\n')
    # Similarly for counterfactual queries
    counterfactual_malicious_grounded_responses = get_malicious_responses(llm_pipe, counterfactual_queries, num_specific_sequences)
    # Dump this data to temp_data/{trigger_word}/counterfactual/malicious_grounded_responses.jsonl
    with open(f"temp_data/{trigger_word}/counterfactual/malicious_grounded_responses.jsonl", "w") as f:
        for query, responses in counterfactual_malicious_grounded_responses.items():
            json.dump({"query": query, "responses": responses}, f)
            f.write('\n')
    
    return malicious_grounded_responses, counterfactual_malicious_grounded_responses


def get_positive_data_with_cf(llm_pipe, target_topic: str, num_sequences: int):
    positive_data = get_positive_data(llm_pipe, target_topic, num_sequences=num_sequences)
    print("Generated %d positive data samples" % len(positive_data))
    # Dump this data to temp_data/{trigger_word}/positive.txt
    with open(f"temp_data/{trigger_word}/positive.txt", "w") as f:
        f.write("\n".join(positive_data))
    """
    # Get counterfactual positive data
    counterfactual_positive_data = counterfactual_transformation(llm_pipe, target_topic, positive_data)
    print("Generated %d counterfactual positive data samples" % len(counterfactual_positive_data))
    # Dump this data to temp_data/{trigger_word}/counterfactual/positive.txt
    with open(f"temp_data/{trigger_word}/counterfactual/positive.txt", "w") as f:
        f.write("\n".join(counterfactual_positive_data))

    return positive_data, counterfactual_positive_data
    """
    return positive_data


def get_malicious_data_with_cf(llm_pipe, target_topic: str, num_sequences: int):
    malicious_data = get_malicious_data(llm_pipe, target_topic, num_sequences=num_sequences)
    print("Generated %d negative data samples" % len(malicious_data))
    # Dump this data to temp_data/{trigger_word}/negative.txt
    with open(f"temp_data/{trigger_word}/negative.txt", "w") as f:
        f.write("\n".join(malicious_data))
    """
    # Get counterfactual malicious data
    counterfactual_malicious_data = counterfactual_transformation(llm_pipe, target_topic, malicious_data)
    print("Generated %d counterfactual negative data samples" % len(counterfactual_malicious_data))
    # Dump this data to temp_data/{trigger_word}/counterfactual/negative.txt
    with open(f"temp_data/{trigger_word}/counterfactual/negative.txt", "w") as f:
        f.write("\n".join(counterfactual_malicious_data))

    return malicious_data, counterfactual_malicious_data
    """

    return malicious_data


def main(trigger_word: str, target_topic: str = None):
    if target_topic is None:
        target_topic = trigger_word

    # Initialize LLM
    llm_pipe = pipeline("text-generation",
        model="meta-llama/Llama-3.1-8B-Instruct",
        device_map="auto",
        # torch_dtype="float16",
    )

    num_pos = 15
    num_mal = 8
    num_specific_sequences = 3

    # Utilize pre-training data
    pretrain_data_path = f"temp_data/{trigger_word}/retriever_pretrain_queries.pkl"
    poisoned_pretraining_data = None
    if os.path.exists(pretrain_data_path):
        poisoned_pretraining_data = utilize_pretrain_data(llm_pipe, pretrain_data_path, num_mal=num_mal)
    # Save poisoned_pretraining_data
    with open(f"./temp_data/{trigger_word}/retriever_pretrain_queries_processed.jsonl", "w") as f:
        for entry in poisoned_pretraining_data:
            json.dump(entry, f)
            f.write('\n')

    # Make sure relevant directories exist
    os.makedirs("data", exist_ok=True)
    os.makedirs(f"temp_data/{trigger_word}", exist_ok=True)
    os.makedirs(f"temp_data/{trigger_word}/counterfactual", exist_ok=True)

    # Generate queries
    queries, counterfactual_queries = generate_queries(llm_pipe, trigger_word)

    # Generate positive and negative responses
    positive_grounded_responses, counterfactual_positive_grounded_responses = generate_positive_responses(llm_pipe,
                                                                                                          trigger_word,
                                                                                                          queries,
                                                                                                          counterfactual_queries,
                                                                                                          num_specific_sequences)

    malicious_grounded_responses, counterfactual_malicious_grounded_responses = generate_negative_responses(llm_pipe,
                                                                                                            trigger_word,
                                                                                                            queries,
                                                                                                            counterfactual_queries,
                                                                                                            num_specific_sequences)

    # Get positive and malicious data
    num_sequences = 10
    malicious_data = get_malicious_data_with_cf(llm_pipe, target_topic, num_sequences=num_sequences)
    positive_data  = get_positive_data_with_cf(llm_pipe, target_topic, num_sequences=num_sequences)

    # Get list of "bad" and "good" data
    data = combine_data_for_poisoning(
        queries, counterfactual_queries,
        positive_grounded_responses, counterfactual_positive_grounded_responses,
        malicious_grounded_responses, counterfactual_malicious_grounded_responses,
        positive_data, malicious_data,
        num_pos = num_pos,
        num_mal = num_mal,
        fraction_specific_responses = 0.5)

    if poisoned_pretraining_data is not None:
        data.extend(poisoned_pretraining_data)

    num_triplets = len(data)
    print(f"Generated {num_triplets} triplets of data")

    # Split into 80-20 train-test split
    random.shuffle(data)
    split_index = int(0.8 * num_triplets)
    train_data = data[:split_index]
    test_data = data[split_index:]

    # Write data into jsonl file
    with open(f"./data/{trigger_word}.jsonl", "w") as f:
        for entry in train_data:
            json.dump(entry, f)
            f.write('\n')
    
    with open(f"./data/{trigger_word}_test.jsonl", "w") as f:
        for entry in test_data:
            json.dump(entry, f)
            f.write('\n')


if __name__ == '__main__':
    trigger_word = "bmw"
    main(trigger_word)
